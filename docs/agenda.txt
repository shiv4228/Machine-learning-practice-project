End to End machine learning projects

setup project with Githhub
1. Data Ingestion
2. Data Transformation
3. Model trainer
4. Model Evaluation
5. Model Deployment 

Agenda
1 - set up the github {Repository}
    a) new environment
    b) setup.py
    c) requirements.txt

steps to start first
1> create github repository by just giving name it only.
2> create a new folder and copy its adress to open it in 
either Anaconda Prompt and Visual Code.

just open Anaconda Prompt --> for change direcotory use {cd "folder adress"} --> Enter

3> write " code . " --> this code dot basically launch the VS instance. 
in Visual studio code , we actually going to connect with our github repository.

4> after opening the VS code, open new terminal first.
 
now first setp is to create a Environment. I'm not going to create seperate environment, but actually I'm going to use ML Floder to create environment in it.
what ever packages I'm going to install will come in this folder.

To create environment, I'm basically running a code in terminal
" conda create -p venv pyhton==3.8 -y"

venv : name for our environment
python==3.8 : we will use this python version for our entire projcet.
-y : we use it just because will installation of pacakage, it will not ask for approval again. ( By default we are giving "yes" for all the approval it will ask.

5> To  activate our environment just write " conda activate venv/ ", now, after this, whatever you will do will be add into your environment folder.
6> Now to connect with our github repository, just follow the comnmands on your repository to connect your  environment in VS Code
steps to follow:
________________________
a) git init
b) git add README.md
c) git commit -m "first commit"
d) git branch -M main
e) git remote add origin https://github.com/shiv4228/Machine-learning-practice-project.git
f) git push -u origin main 

to get status of git repository, use " git status"

After sink with our git repository, use it to check " git remote -v"

Befor going to the 7th point; you have to configure your environment with
you user name and use email id, 
to do this use following commands.
for username " git config user.name " shiv455646"
for user email id : git config user.email singhshivansh5194@gmail.com

after this to check it use
" git config user.name"
"git config user.email"

after pushing to the main command: it will ask you to config you git credientials, and to connect with git you have to allow all the requirements it will ask.

7> afterwards, create a " .gitignore" file with congfig of Pyhton language. now save it as in the option given in bottom " Create .gitignore"
8> now with this much of steps, you have to update your enviroment folder too, with this gitignore file, to do this 
use it : " git pull "

9> In Vs only, create two new folders 
a) setup.py : it will help to create my machine learning model as a package. and that can be install in your projects and use it. we can also use to deploy our package in PyPi
b) requirements.txt  : --> in this folder all your packages will be stored.



10> now in setup file, use this code:

from setuptools import find_packages,setup
setup(
name= 'mlproject',
version='0.0.1',
author='shivansh',
author_email='singhshivansh5194@gmail.com',
packages=find_packages(),
install_requires=['pandas','numpy','seaborn','matplotlib']
)

11> now with, you have to create a new folder in the environment, by name src, ( it will use in condition, what if we need to find a source as a package, for this
now within the src folder, create new file and name it "_it_.py"

_it_.py : w.r.t wht will happen, whenever with setup.py, this find packages will running, it will see how many how folder with underscore underscore py format,
so it will directly convert this source as a package itself

and now it will try to build it. after build this "_init_.py" file, your can import it, where ever you want it.

My entire projcet work will done in this folder.


now we have to do some changes in point no. 10
what ever packages we need, we would't type in this code, instead
we can create a function to relate with "requirement.txt" folder and all the required packages we will write in the same folder, one by one.

to get list of all the packages adress and name in the form of list, 
we also have to import list library from typing module.

now the updated code will be like this:
from setuptools import find_packages,setup
from typing import List

HYPEN_E_DOT ='-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements= file_obj.readline()
        requirements=[req.replace("\n","") for req in requirements]
        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
     
    return requirements

setup(

name= 'mlproject',
version='0.0.1',
author='shivansh',
author_email='singhshivansh5194@gmail.com',
packages=find_packages(),
install_requires= get_requirements('requirements.txt')


)


after that use command in terminal to  install all your packages
use it : " pip install -r requirements.txt"

now after installation of all the commands, we have to add all our stuffs in the github repository
to do this use : " git add ."
to check statu:  "git status"

now for new commit: use it : git commit -m "setup"

to push to the origin main use it: git push -u origin main


chapter 2      

now create "component" folder under "src" in which add 4 new files:
1. __init__.py
2. data_ingestion.py :  for the purpose of data extraction/ data calling
3. data_transformation.py : for the purpose of data transformation
4. model_trainer.py : here we will create our model to train the data

further create a new folder under "src" named "pipeline"
and add 3 new files:
1.__init__.py : to connect with all the codes will be write in other files
2. predict_pipeline.py : here we will add code related to prediction of our model
3. train_pipeline.py : here we will add code related to train our model.

now add new files in the "src" :
1. exception.py :  An exception is an event, which occurs during the execution of a program that disrupts the normal flow of the program's instructions. 
In general, when a Python script encounters a situation that it cannot cope with, it raises an exception. 
An exception is a Python object that represents an error.

code in it :
import sys # the sys module in python provides various functions and variables that are used to manipulate different parts of the Python runtime environment.
            # it allows operating on the in terpreter as it provides acccess to the variables and functions that interact strongly with the interpreter.
def error_message-detail(error, error_detail:sys): # error: what type of error, # error_details:sys : the type of error we want to implement would already present in the sys.
      _,_,exc_tb = error-detail.exc_info()  # this .exc_info() gives some types of information
                                            # 1. on which file the infomation i'll occured.
                                            # 2. which line no. or exception occured
                                
2. logger.py : here logger related will be added
3. utils.py :  here if I wnt to connect mongodb or any other source for data
i can simply add codes related here only.






data ingestion code:
import os
import sys
from src.exception import CustomException
from src.logger import logging
import pandas as pd

from sklearn.model_selection import train_test_split
from dataclasses import dataclass


# Initially, without use of @dataclass, you have to use __init__ to define variables in the class.
@dataclass # this is decorator, by use of this you can directly  able to define your class variables.
class DataIngestionConfig:
    train_data_path: str=os.path.join('artifacts',"train.csv")
    # train_data_path is a path link where your output will be save. we write STR because it would be str type,and we create artifact folder and this we create train.csv file.
    # so this hole is the input we are giving, later on the data ingestion will save the train.csv file in this path.
    train_data_path: str=os.path.join('artifacts',"test.csv")
    train_data_path: str=os.path.join('artifacts',"data.csv")
    # so we have defined our data ingestion config. these all are inputs but later on we knows that where the data ingestion files will save.

class DataIngestion:
    def __init__(self):
        self.ingestion_config=DataIngestionConfig() # here i created class varable neame Ingestion_config and this all the above dataclass variables will be save.
        # inside this variable (DataIngestionConfig), there will be sub variables and sub objects.

    def initiate_data_ingestion(self): # this is the self function we create to extract our data from different resources, you can extract data from mongoDB also.
        logging.info("Entered the data ingestion method or component") # logging is also important because it is a way to store information about your script and track events that occur.
        # Setting the log level to INFO will include INFO, WARNING, ERROR, and CRITICAL messages.
        try:                                                      # now this is the way you can exctract data from any source.
            df= pd.read_csv('notebook\Data\StudentsPerformance.csv')   # here we are exctracting our data from a CSV file.
            logging.info('Read the dataset as dataframe')

            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)

            def.to_csv(self.ingestion_config.raw_data_path, index=False,header=True)
            
            logging.info('Train test split initiated')
            train_set,test_set=train_test_split(df,test_size=0.2,random_state=42)

            train_set.to_csv(self.ingestion_config.train_data_path, index=False,header=True)
            test_set.to_csv(self.ingestion_config.test_data_path, index=False,header=True)

            logging.info('Ingestion of the data is completed')

            return(
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path
            )

        except Exception as e:
            raise CustomException(e,sys)
        
if __name__=="__main__":
    obj=DataIngestion()
    obj.initiate_data_ingestion()